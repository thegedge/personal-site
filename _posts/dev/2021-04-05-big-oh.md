---
title: Big O? Big "ohhhhhh" üí°
category: dev
tags: [theory]
description: >-
  A practical introduction to big O notation. What is it? What's it good for? We'll dive into a
  practical approach to understand big O notation, and how we can use it in our daily lives
  effectively.
additional_scripts:
  - https://d3js.org/d3.v6.min.js
  - /js/bigO.js
---

So you've just heard about this thing called big O. Maybe in some class you're taking on algorithmic
complexity, or maybe someone asked you about the big O of the code you just wrote on a whiteboard
(ugh, I know). You're not quite sure how this theoretical concept applies to your job, and you want
to learn more. Well, you've come to the right place! üôå

<div class="p-4 bg-blue-100 rounded border-blue-200 border-2 mx-4">
  <h2 class="text-xl text-blue-600 font-bold m-0">Summary</h2>
  <ul class="ml-8 list-summary">
    <li>Big O is one of many ways to describe the growth of a function.</li>
    <li>In particular, it describes an upper bound.</li>
    <li>It hides information that is important for practical considerations.</li>
  </ul>
</div>

## üìñ Story time

Let me start with a short story of how to _not_ use big O. Early in my career, I was building out a
remote development environment for the company I worked for. One part of this system required
checking inclusion in a collection of things. I decided to use an array over a set, knowing the size
of this collection was never going to be large. Given the small input size and cache-friendly nature
of arrays, I decided it was the better option.

This one decision resulted in what was one of the longest, most difficult code reviews I had to work
through, focused primarily on my tech lead wanting me to use a set over an array. They showed a
clear lack of understanding of big O notation, arguing that I should make the change solely on the
fact that inclusion checks in a set are $O(\text{lg } n)$ whereas inclusion in an array is $O(n)$.
Even after showing a benchmark indicating the array performed significantly better for the input
sizes we were to expect for the foreseeable future, they were hesitant to give me the thumbs up.

It was that experience that told me I should write this blog post (it also taught me about what not
to do in a code review). Let's dive in!

## ü§î What is big O?

Suppose we've implemented some function $f$ that computes a value from some input of size $n$, and
we want to understand how that function performs as $n$ gets bigger. There are many ways to
understand growth, and big O is one of them. Big O defines an upper bound on the growth of a
function, what we like to think of as the "worst case" scenario.

Formally, we say that $f$ is big O of some other function $g$ ‚Äî written $f(n) = O(g(n))$ ‚Äî if, for
some input size $n$:

$$
f(n) <= C g(n) \quad \text{for some constant } C, \text{for all } n > n_0
$$

That's a whole lot of math, but it boils down to one thing: $f$ is $O(g)$ if $g$ is _always_ bigger
than $f$ _after_ some input size.

## üßê What does it look like?

Let's take a look at some functions:

<div>
  <figure style="overflow-x: auto">
    <svg id="bigO_graph1" style="width: 40em; height: 20em; margin: auto"></svg>
  </figure>
</div>

Note $lg$ is the [base 2 log](https://en.wikipedia.org/wiki/Binary_logarithm), also written as
$log_2$. We can see how these functions grow, and how some grow much faster than others. Where would
we see these in practice?

- $O(\lg{n})$: a [divide and conquer](https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm)
  search, like [binary search](https://en.wikipedia.org/wiki/Binary_search_algorithm). Known as
  logarithmic complexity.
- $O(n)$: a single loop over an array, like computing a sum. Known as linear complexity.
- $O(n \cdot \lg{n})$: a divide and conquer approach where each "division" involves looping over the
  divided part, like [merge sort](https://en.wikipedia.org/wiki/Merge_sort).
- $O(n^2)$: a nested loop over the array, like in a simple sort algorithm, such as
  [bubble sort](https://en.wikipedia.org/wiki/Bubble_sort). Known as quadratic complexity.

Remember that $n_0$ term from the definition above, the point at which the big O function actually
becomes an upper bound? That's _really_ important to remember. We generally consider $O(n)$ to be
better than $O(n^2)$, but that's only true after a certain point. Consider the following:

<div>
  <figure style="overflow-x: auto">
    <svg id="bigO_graph2" style="width: 40em; height: 20em; margin: auto"></svg>
  </figure>
</div>

Suppose these two functions describe the performance of two different algorithms we have to choose
between, parameterized on the size $n$ of an input array. If our input array has less than 800
elements, the function that has quadratic complexity actually performs better! üôÄ

## üë∑‚Äç‚ôÄÔ∏è But how can I use this?

There are three practical considerations when incorporating big O into your work.

First, and foremost, we should **consider our input sizes**. If the size of our inputs will remain
small or fixed, big O is not as useful a tool. With small inputs, things like
[CPU caches](https://en.wikipedia.org/wiki/CPU_cache) will likely have a significant impact. This
point is _really_ important to remember because it is often overlooked.

Big O is a great tool to use to compare two approaches when we do have large, arbitrary input sizes.
It's important to know that big O is also an upper bound, but not necessarily a _tight_ upper bound.
What that means is that a function that is $O(n^2)$ _could_ be $O(n)$, but no one has proven that
yet.

The last consideration is to **benchmark our code** to prove that one approach is better than
another. In particular, benchmarking over various input sizes and shapes will give us confidence
we've made the right decision.
